{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - S&OP Dataset\n",
    "\n",
    "**Sales & Operations Planning - Comprehensive Data Exploration**\n",
    "\n",
    "Author: Aviwe Dlepu\n",
    "Date: October 2025\n",
    "\n",
    "## Objective\n",
    "Conduct thorough exploratory data analysis on S&OP datasets to:\n",
    "- Understand data structure and quality\n",
    "- Identify patterns and relationships\n",
    "- Detect anomalies and outliers\n",
    "- Generate actionable insights\n",
    "- Inform modeling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import missingno as msno\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AD Solutions color scheme\n",
    "AD_COLORS = {\n",
    "    'primary': '#CC5500',\n",
    "    'secondary': '#B8470B',\n",
    "    'accent': '#FF6347',\n",
    "    'highlight': '#FFA500',\n",
    "    'neutral': '#8B4513'\n",
    "}\n",
    "\n",
    "print(\"üîç EDA Environment Setup Complete\")\n",
    "print(\"Project: S&OP Exploratory Data Analysis\")\n",
    "print(\"Author: Aviwe Dlepu\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with error handling\n",
    "datasets = {}\n",
    "file_mapping = {\n",
    "    'orders': 'final_orders_perfect_integrity.csv',\n",
    "    'products': 'final_products_perfect_integrity.csv',\n",
    "    'inventory': 'final_inventory_perfect_integrity.csv',\n",
    "    'forecasts': 'final_forecasts_perfect_integrity.csv'\n",
    "}\n",
    "\n",
    "print(\"üìÇ Loading S&OP Datasets...\")\n",
    "for dataset_name, filename in file_mapping.items():\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        datasets[dataset_name] = df\n",
    "        print(f\"‚úÖ {dataset_name.capitalize()}: {len(df):,} records, {len(df.columns)} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dataset_name}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")\n",
    "\n",
    "# Quick overview\n",
    "if datasets:\n",
    "    total_records = sum(len(df) for df in datasets.values())\n",
    "    print(f\"üìà Total records across all datasets: {total_records:,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No datasets loaded. Please check file paths and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset overview\n",
    "def analyze_dataset_structure(df, dataset_name):\n",
    "    print(f\"\\n{'='*20} {dataset_name.upper()} DATASET ANALYSIS {'='*20}\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_pct = (missing_values / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing_Count': missing_values,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    })\n",
    "    missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_summary) > 0:\n",
    "        print(f\"\\n‚ùå Missing Values Found:\")\n",
    "        print(missing_summary.round(2))\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No missing values detected\")\n",
    "    \n",
    "    # Duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates:,} ({(duplicates/len(df)*100):.2f}%)\")\n",
    "    \n",
    "    # Unique values per column\n",
    "    print(f\"\\nüî¢ Unique Values per Column:\")\n",
    "    for col in df.columns[:10]:  # Show first 10 columns\n",
    "        unique_count = df[col].nunique()\n",
    "        unique_pct = (unique_count / len(df)) * 100\n",
    "        print(f\"   {col}: {unique_count:,} ({unique_pct:.1f}%)\")\n",
    "    \n",
    "    if len(df.columns) > 10:\n",
    "        print(f\"   ... and {len(df.columns) - 10} more columns\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "# Analyze each dataset\n",
    "missing_data_summary = {}\n",
    "for name, df in datasets.items():\n",
    "    missing_summary = analyze_dataset_structure(df, name)\n",
    "    missing_data_summary[name] = missing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality visualization\n",
    "if datasets:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    fig.suptitle('üîç Data Quality Overview - S&OP Datasets', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    dataset_names = list(datasets.keys())\n",
    "    \n",
    "    for i, (name, df) in enumerate(datasets.items()):\n",
    "        if i < 4:  # Limit to 4 datasets\n",
    "            row, col = i // 2, i % 2\n",
    "            \n",
    "            # Missing data visualization\n",
    "            missing_counts = df.isnull().sum()\n",
    "            if missing_counts.sum() > 0:\n",
    "                missing_counts[missing_counts > 0].plot(kind='bar', ax=axes[row, col], \n",
    "                                                       color=AD_COLORS['primary'])\n",
    "                axes[row, col].set_title(f'{name.capitalize()} - Missing Values')\n",
    "                axes[row, col].set_ylabel('Missing Count')\n",
    "                axes[row, col].tick_params(axis='x', rotation=45)\n",
    "            else:\n",
    "                axes[row, col].text(0.5, 0.5, f'{name.capitalize()}\\n‚úÖ No Missing Values', \n",
    "                                  ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "                axes[row, col].set_xlim(0, 1)\n",
    "                axes[row, col].set_ylim(0, 1)\n",
    "                axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Dataset size comparison\n",
    "    dataset_sizes = {name: len(df) for name, df in datasets.items()}\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    bars = ax.bar(dataset_sizes.keys(), dataset_sizes.values(), \n",
    "                  color=[AD_COLORS['primary'], AD_COLORS['secondary'], \n",
    "                        AD_COLORS['accent'], AD_COLORS['highlight']])\n",
    "    \n",
    "    ax.set_title('üìä Dataset Size Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Records')\n",
    "    ax.set_xlabel('Dataset')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Orders Dataset Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders'].copy()\n",
    "    \n",
    "    print(\"üì¶ ORDERS DATASET ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Convert date column\n",
    "    if 'order_date' in orders_df.columns:\n",
    "        orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "        print(f\"üìÖ Date Range: {orders_df['order_date'].min()} to {orders_df['order_date'].max()}\")\n",
    "        print(f\"üìà Time Span: {(orders_df['order_date'].max() - orders_df['order_date'].min()).days} days\")\n",
    "    \n",
    "    # Revenue metrics\n",
    "    if 'order_value' in orders_df.columns:\n",
    "        total_revenue = orders_df['order_value'].sum()\n",
    "        avg_order_value = orders_df['order_value'].mean()\n",
    "        median_order_value = orders_df['order_value'].median()\n",
    "        \n",
    "        print(f\"\\nüí∞ Revenue Metrics:\")\n",
    "        print(f\"   Total Revenue: R{total_revenue:,.2f}\")\n",
    "        print(f\"   Average Order Value: R{avg_order_value:,.2f}\")\n",
    "        print(f\"   Median Order Value: R{median_order_value:,.2f}\")\n",
    "        print(f\"   Revenue Std Dev: R{orders_df['order_value'].std():,.2f}\")\n",
    "    \n",
    "    # Customer segments\n",
    "    if 'customer_type' in orders_df.columns:\n",
    "        customer_dist = orders_df['customer_type'].value_counts()\n",
    "        print(f\"\\nüë• Customer Type Distribution:\")\n",
    "        for customer_type, count in customer_dist.items():\n",
    "            pct = (count / len(orders_df)) * 100\n",
    "            print(f\"   {customer_type}: {count:,} orders ({pct:.1f}%)\")\n",
    "    \n",
    "    # Regional distribution\n",
    "    if 'region' in orders_df.columns:\n",
    "        regional_dist = orders_df['region'].value_counts()\n",
    "        print(f\"\\nüåç Regional Distribution:\")\n",
    "        for region, count in regional_dist.items():\n",
    "            pct = (count / len(orders_df)) * 100\n",
    "            print(f\"   {region}: {count:,} orders ({pct:.1f}%)\")\n",
    "    \n",
    "    # Product categories\n",
    "    if 'category' in orders_df.columns:\n",
    "        category_dist = orders_df['category'].value_counts()\n",
    "        print(f\"\\nüè∑Ô∏è Product Category Distribution:\")\n",
    "        for category, count in category_dist.head().items():\n",
    "            pct = (count / len(orders_df)) * 100\n",
    "            print(f\"   {category}: {count:,} orders ({pct:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Orders dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders data visualizations\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders'].copy()\n",
    "    \n",
    "    if 'order_date' in orders_df.columns:\n",
    "        orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('üì¶ Orders Dataset - Exploratory Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Order value distribution\n",
    "    if 'order_value' in orders_df.columns:\n",
    "        axes[0, 0].hist(orders_df['order_value'], bins=50, color=AD_COLORS['primary'], alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].set_title('Order Value Distribution')\n",
    "        axes[0, 0].set_xlabel('Order Value (R)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(orders_df['order_value'].mean(), color='red', linestyle='--', label='Mean')\n",
    "        axes[0, 0].axvline(orders_df['order_value'].median(), color='orange', linestyle='--', label='Median')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Customer type breakdown\n",
    "    if 'customer_type' in orders_df.columns:\n",
    "        customer_counts = orders_df['customer_type'].value_counts()\n",
    "        axes[0, 1].pie(customer_counts.values, labels=customer_counts.index, autopct='%1.1f%%',\n",
    "                      colors=[AD_COLORS['primary'], AD_COLORS['secondary'], AD_COLORS['accent']])\n",
    "        axes[0, 1].set_title('Customer Type Distribution')\n",
    "    \n",
    "    # 3. Regional performance\n",
    "    if 'region' in orders_df.columns and 'order_value' in orders_df.columns:\n",
    "        regional_revenue = orders_df.groupby('region')['order_value'].sum().sort_values()\n",
    "        axes[0, 2].barh(regional_revenue.index, regional_revenue.values, color=AD_COLORS['highlight'])\n",
    "        axes[0, 2].set_title('Revenue by Region')\n",
    "        axes[0, 2].set_xlabel('Total Revenue (R)')\n",
    "    \n",
    "    # 4. Monthly trend\n",
    "    if 'order_date' in orders_df.columns and 'order_value' in orders_df.columns:\n",
    "        monthly_revenue = orders_df.groupby(orders_df['order_date'].dt.to_period('M'))['order_value'].sum()\n",
    "        axes[1, 0].plot(monthly_revenue.index.astype(str), monthly_revenue.values, \n",
    "                       marker='o', color=AD_COLORS['primary'], linewidth=3)\n",
    "        axes[1, 0].set_title('Monthly Revenue Trend')\n",
    "        axes[1, 0].set_ylabel('Revenue (R)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Quantity distribution\n",
    "    if 'qty' in orders_df.columns:\n",
    "        axes[1, 1].hist(orders_df['qty'], bins=30, color=AD_COLORS['secondary'], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_title('Quantity Distribution')\n",
    "        axes[1, 1].set_xlabel('Quantity')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 6. Discount analysis\n",
    "    if 'discount_pct' in orders_df.columns:\n",
    "        discount_data = orders_df[orders_df['discount_pct'] > 0]['discount_pct']\n",
    "        if len(discount_data) > 0:\n",
    "            axes[1, 2].hist(discount_data, bins=20, color=AD_COLORS['accent'], alpha=0.7, edgecolor='black')\n",
    "            axes[1, 2].set_title('Discount Distribution (Non-Zero)')\n",
    "            axes[1, 2].set_xlabel('Discount %')\n",
    "            axes[1, 2].set_ylabel('Frequency')\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, 'No Discounts\\nFound', ha='center', va='center')\n",
    "            axes[1, 2].set_xlim(0, 1)\n",
    "            axes[1, 2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis & Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numeric columns\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    \n",
    "    print(\"üìä STATISTICAL SUMMARY - ORDERS DATASET\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_columns = orders_df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nüî¢ Numeric Columns Analysis:\")\n",
    "    \n",
    "    stats_summary = orders_df[numeric_columns].describe()\n",
    "    print(stats_summary.round(2))\n",
    "    \n",
    "    # Correlation analysis\n",
    "    if len(numeric_columns) > 1:\n",
    "        print(f\"\\nüîó Correlation Matrix:\")\n",
    "        correlation_matrix = orders_df[numeric_columns].corr()\n",
    "        print(correlation_matrix.round(3))\n",
    "        \n",
    "        # Strong correlations\n",
    "        print(f\"\\nüí™ Strong Correlations (|r| > 0.7):\")\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_value = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_value) > 0.7:\n",
    "                    var1 = correlation_matrix.columns[i]\n",
    "                    var2 = correlation_matrix.columns[j]\n",
    "                    print(f\"   {var1} ‚Üî {var2}: {corr_value:.3f}\")\n",
    "    \n",
    "    # Outlier detection for key metrics\n",
    "    if 'order_value' in orders_df.columns:\n",
    "        Q1 = orders_df['order_value'].quantile(0.25)\n",
    "        Q3 = orders_df['order_value'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold_low = Q1 - 1.5 * IQR\n",
    "        outlier_threshold_high = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = orders_df[(orders_df['order_value'] < outlier_threshold_low) | \n",
    "                           (orders_df['order_value'] > outlier_threshold_high)]\n",
    "        \n",
    "        print(f\"\\nüéØ Order Value Outliers:\")\n",
    "        print(f\"   Total outliers: {len(outliers):,} ({len(outliers)/len(orders_df)*100:.1f}%)\")\n",
    "        print(f\"   Lower threshold: R{outlier_threshold_low:,.2f}\")\n",
    "        print(f\"   Upper threshold: R{outlier_threshold_high:,.2f}\")\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            print(f\"   Top 5 highest value orders:\")\n",
    "            top_orders = orders_df.nlargest(5, 'order_value')[['order_id', 'order_value', 'customer_type', 'region']]\n",
    "            print(top_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    numeric_columns = orders_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_columns) > 1:\n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        correlation_matrix = orders_df[numeric_columns].corr()\n",
    "        \n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title('üîó Orders Dataset - Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Box plots for categorical variables\n",
    "    if 'customer_type' in orders_df.columns and 'order_value' in orders_df.columns:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Order value by customer type\n",
    "        orders_df.boxplot(column='order_value', by='customer_type', ax=axes[0])\n",
    "        axes[0].set_title('Order Value by Customer Type')\n",
    "        axes[0].set_xlabel('Customer Type')\n",
    "        axes[0].set_ylabel('Order Value (R)')\n",
    "        \n",
    "        # Order value by region\n",
    "        if 'region' in orders_df.columns:\n",
    "            orders_df.boxplot(column='order_value', by='region', ax=axes[1])\n",
    "            axes[1].set_title('Order Value by Region')\n",
    "            axes[1].set_xlabel('Region')\n",
    "            axes[1].set_ylabel('Order Value (R)')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.suptitle('üìä Order Value Distribution by Categories', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inventory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'inventory' in datasets:\n",
    "    inventory_df = datasets['inventory'].copy()\n",
    "    \n",
    "    print(\"üìã INVENTORY DATASET ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Inventory value metrics\n",
    "    if 'inventory_value' in inventory_df.columns:\n",
    "        total_inventory = inventory_df['inventory_value'].sum()\n",
    "        avg_inventory = inventory_df['inventory_value'].mean()\n",
    "        median_inventory = inventory_df['inventory_value'].median()\n",
    "        \n",
    "        print(f\"üí∞ Inventory Value Metrics:\")\n",
    "        print(f\"   Total Inventory Value: R{total_inventory:,.2f}\")\n",
    "        print(f\"   Average per Product: R{avg_inventory:,.2f}\")\n",
    "        print(f\"   Median per Product: R{median_inventory:,.2f}\")\n",
    "    \n",
    "    # Stock status distribution\n",
    "    if 'stock_status' in inventory_df.columns:\n",
    "        stock_dist = inventory_df['stock_status'].value_counts()\n",
    "        print(f\"\\nüìä Stock Status Distribution:\")\n",
    "        for status, count in stock_dist.items():\n",
    "            pct = (count / len(inventory_df)) * 100\n",
    "            print(f\"   {status}: {count:,} products ({pct:.1f}%)\")\n",
    "    \n",
    "    # Warehouse distribution\n",
    "    if 'warehouse' in inventory_df.columns:\n",
    "        warehouse_dist = inventory_df['warehouse'].value_counts()\n",
    "        print(f\"\\nüè¢ Warehouse Distribution:\")\n",
    "        for warehouse, count in warehouse_dist.items():\n",
    "            pct = (count / len(inventory_df)) * 100\n",
    "            print(f\"   {warehouse}: {count:,} products ({pct:.1f}%)\")\n",
    "    \n",
    "    # High-value inventory items\n",
    "    if 'inventory_value' in inventory_df.columns:\n",
    "        top_inventory = inventory_df.nlargest(10, 'inventory_value')\n",
    "        print(f\"\\nüíé Top 10 High-Value Inventory Items:\")\n",
    "        for _, row in top_inventory.iterrows():\n",
    "            print(f\"   {row['product_id']}: R{row['inventory_value']:,.2f} ({row.get('stock_status', 'N/A')})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Inventory dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inventory visualizations\n",
    "if 'inventory' in datasets:\n",
    "    inventory_df = datasets['inventory']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üìã Inventory Dataset - Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Inventory value distribution\n",
    "    if 'inventory_value' in inventory_df.columns:\n",
    "        axes[0, 0].hist(inventory_df['inventory_value'], bins=50, color=AD_COLORS['primary'], \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].set_title('Inventory Value Distribution')\n",
    "        axes[0, 0].set_xlabel('Inventory Value (R)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(inventory_df['inventory_value'].mean(), color='red', \n",
    "                          linestyle='--', label='Mean')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Stock status pie chart\n",
    "    if 'stock_status' in inventory_df.columns:\n",
    "        stock_counts = inventory_df['stock_status'].value_counts()\n",
    "        axes[0, 1].pie(stock_counts.values, labels=stock_counts.index, autopct='%1.1f%%',\n",
    "                      colors=[AD_COLORS['primary'], AD_COLORS['secondary'], AD_COLORS['accent']])\n",
    "        axes[0, 1].set_title('Stock Status Distribution')\n",
    "    \n",
    "    # 3. Available quantity distribution\n",
    "    if 'available_qty' in inventory_df.columns:\n",
    "        axes[1, 0].hist(inventory_df['available_qty'], bins=40, color=AD_COLORS['secondary'], \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_title('Available Quantity Distribution')\n",
    "        axes[1, 0].set_xlabel('Available Quantity')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Inventory value by stock status\n",
    "    if 'stock_status' in inventory_df.columns and 'inventory_value' in inventory_df.columns:\n",
    "        status_value = inventory_df.groupby('stock_status')['inventory_value'].sum()\n",
    "        axes[1, 1].bar(status_value.index, status_value.values, \n",
    "                      color=[AD_COLORS['primary'], AD_COLORS['accent'], AD_COLORS['highlight']])\n",
    "        axes[1, 1].set_title('Total Inventory Value by Stock Status')\n",
    "        axes[1, 1].set_xlabel('Stock Status')\n",
    "        axes[1, 1].set_ylabel('Total Value (R)')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Forecast Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'forecasts' in datasets:\n",
    "    forecasts_df = datasets['forecasts'].copy()\n",
    "    \n",
    "    print(\"üéØ FORECAST PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Convert date column\n",
    "    if 'forecast_month' in forecasts_df.columns:\n",
    "        forecasts_df['forecast_month'] = pd.to_datetime(forecasts_df['forecast_month'])\n",
    "    \n",
    "    # Calculate absolute error\n",
    "    if 'variance_pct' in forecasts_df.columns:\n",
    "        forecasts_df['absolute_error'] = abs(forecasts_df['variance_pct'])\n",
    "        forecasts_df['forecast_accuracy'] = 100 - forecasts_df['absolute_error']\n",
    "    \n",
    "    # Overall accuracy metrics\n",
    "    if 'absolute_error' in forecasts_df.columns:\n",
    "        overall_accuracy = 100 - forecasts_df['absolute_error'].mean()\n",
    "        median_accuracy = 100 - forecasts_df['absolute_error'].median()\n",
    "        \n",
    "        print(f\"üìä Overall Forecast Performance:\")\n",
    "        print(f\"   Mean Accuracy: {overall_accuracy:.2f}%\")\n",
    "        print(f\"   Median Accuracy: {median_accuracy:.2f}%\")\n",
    "        print(f\"   Standard Deviation: {forecasts_df['absolute_error'].std():.2f}%\")\n",
    "    \n",
    "    # Performance by forecast type\n",
    "    if 'forecast_type' in forecasts_df.columns and 'absolute_error' in forecasts_df.columns:\n",
    "        type_performance = forecasts_df.groupby('forecast_type').agg({\n",
    "            'absolute_error': ['mean', 'median', 'std', 'count'],\n",
    "            'forecast_accuracy': ['mean', 'median']\n",
    "        }).round(2)\n",
    "        \n",
    "        print(f\"\\nü§ñ Performance by Forecast Type:\")\n",
    "        print(type_performance)\n",
    "    \n",
    "    # Performance by confidence level\n",
    "    if 'confidence_level' in forecasts_df.columns and 'absolute_error' in forecasts_df.columns:\n",
    "        confidence_performance = forecasts_df.groupby('confidence_level').agg({\n",
    "            'absolute_error': ['mean', 'count'],\n",
    "            'forecast_accuracy': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(f\"\\nüìà Performance by Confidence Level:\")\n",
    "        print(confidence_performance)\n",
    "    \n",
    "    # Best and worst forecasts\n",
    "    if 'absolute_error' in forecasts_df.columns:\n",
    "        best_forecasts = forecasts_df.nsmallest(5, 'absolute_error')\n",
    "        worst_forecasts = forecasts_df.nlargest(5, 'absolute_error')\n",
    "        \n",
    "        print(f\"\\nüèÜ Best 5 Forecasts (Lowest Error):\")\n",
    "        for _, row in best_forecasts.iterrows():\n",
    "            print(f\"   {row['product_id']}: {row['absolute_error']:.1f}% error ({row.get('forecast_type', 'N/A')})\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è Worst 5 Forecasts (Highest Error):\")\n",
    "        for _, row in worst_forecasts.iterrows():\n",
    "            print(f\"   {row['product_id']}: {row['absolute_error']:.1f}% error ({row.get('forecast_type', 'N/A')})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Forecasts dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast performance visualizations\n",
    "if 'forecasts' in datasets:\n",
    "    forecasts_df = datasets['forecasts'].copy()\n",
    "    \n",
    "    if 'variance_pct' in forecasts_df.columns:\n",
    "        forecasts_df['absolute_error'] = abs(forecasts_df['variance_pct'])\n",
    "        forecasts_df['forecast_accuracy'] = 100 - forecasts_df['absolute_error']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üéØ Forecast Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Forecast error distribution\n",
    "    if 'absolute_error' in forecasts_df.columns:\n",
    "        axes[0, 0].hist(forecasts_df['absolute_error'], bins=40, color=AD_COLORS['primary'], \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].set_title('Forecast Error Distribution')\n",
    "        axes[0, 0].set_xlabel('Absolute Error (%)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(forecasts_df['absolute_error'].mean(), color='red', \n",
    "                          linestyle='--', label=f\"Mean: {forecasts_df['absolute_error'].mean():.1f}%\")\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Performance by forecast type\n",
    "    if 'forecast_type' in forecasts_df.columns and 'forecast_accuracy' in forecasts_df.columns:\n",
    "        type_accuracy = forecasts_df.groupby('forecast_type')['forecast_accuracy'].mean()\n",
    "        axes[0, 1].bar(type_accuracy.index, type_accuracy.values, \n",
    "                      color=[AD_COLORS['primary'], AD_COLORS['secondary'], AD_COLORS['accent']])\n",
    "        axes[0, 1].set_title('Average Accuracy by Forecast Type')\n",
    "        axes[0, 1].set_xlabel('Forecast Type')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Forecast vs Actual scatter plot\n",
    "    if 'forecast_qty' in forecasts_df.columns and 'actual_demand' in forecasts_df.columns:\n",
    "        sample_size = min(1000, len(forecasts_df))  # Sample for readability\n",
    "        sample_df = forecasts_df.sample(sample_size)\n",
    "        \n",
    "        axes[1, 0].scatter(sample_df['forecast_qty'], sample_df['actual_demand'], \n",
    "                          alpha=0.6, color=AD_COLORS['primary'])\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        max_val = max(forecasts_df['forecast_qty'].max(), forecasts_df['actual_demand'].max())\n",
    "        axes[1, 0].plot([0, max_val], [0, max_val], 'r--', label='Perfect Prediction')\n",
    "        \n",
    "        axes[1, 0].set_title('Forecast vs Actual Demand')\n",
    "        axes[1, 0].set_xlabel('Forecast Quantity')\n",
    "        axes[1, 0].set_ylabel('Actual Demand')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Performance by confidence level\n",
    "    if 'confidence_level' in forecasts_df.columns and 'absolute_error' in forecasts_df.columns:\n",
    "        forecasts_df.boxplot(column='absolute_error', by='confidence_level', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Error Distribution by Confidence Level')\n",
    "        axes[1, 1].set_xlabel('Confidence Level')\n",
    "        axes[1, 1].set_ylabel('Absolute Error (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset insights\n",
    "print(\"üîó CROSS-DATASET ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "cross_insights = {}\n",
    "\n",
    "# Orders vs Inventory analysis\n",
    "if 'orders' in datasets and 'inventory' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    inventory_df = datasets['inventory']\n",
    "    \n",
    "    # Product demand vs inventory levels\n",
    "    product_demand = orders_df.groupby('product_id').agg({\n",
    "        'qty': 'sum',\n",
    "        'order_value': 'sum',\n",
    "        'order_id': 'count'\n",
    "    }).rename(columns={'qty': 'total_demand', 'order_value': 'total_revenue', 'order_id': 'order_count'})\n",
    "    \n",
    "    # Merge with inventory\n",
    "    demand_inventory = product_demand.merge(\n",
    "        inventory_df[['product_id', 'available_qty', 'inventory_value', 'stock_status']], \n",
    "        on='product_id', how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate inventory turnover\n",
    "    demand_inventory['turnover_ratio'] = demand_inventory['total_revenue'] / demand_inventory['inventory_value']\n",
    "    demand_inventory['demand_coverage'] = demand_inventory['available_qty'] / demand_inventory['total_demand']\n",
    "    \n",
    "    print(f\"üìä Orders vs Inventory Insights:\")\n",
    "    print(f\"   Products in both datasets: {len(demand_inventory):,}\")\n",
    "    print(f\"   Average turnover ratio: {demand_inventory['turnover_ratio'].mean():.2f}\")\n",
    "    print(f\"   Average demand coverage: {demand_inventory['demand_coverage'].mean():.2f}\")\n",
    "    \n",
    "    # High turnover products\n",
    "    high_turnover = demand_inventory.nlargest(5, 'turnover_ratio')\n",
    "    print(f\"\\nüî• Top 5 High Turnover Products:\")\n",
    "    for _, row in high_turnover.iterrows():\n",
    "        print(f\"   {row.name}: {row['turnover_ratio']:.2f} ratio ({row['stock_status']})\")\n",
    "    \n",
    "    # Low turnover, high inventory\n",
    "    low_turnover_high_inventory = demand_inventory[\n",
    "        (demand_inventory['turnover_ratio'] < 1) & \n",
    "        (demand_inventory['inventory_value'] > demand_inventory['inventory_value'].quantile(0.75))\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Low Turnover, High Inventory Products: {len(low_turnover_high_inventory):,}\")\n",
    "    if len(low_turnover_high_inventory) > 0:\n",
    "        total_tied_up = low_turnover_high_inventory['inventory_value'].sum()\n",
    "        print(f\"   Total value tied up: R{total_tied_up:,.2f}\")\n",
    "    \n",
    "    cross_insights['demand_inventory'] = demand_inventory\n",
    "\n",
    "# Orders vs Forecasts analysis\n",
    "if 'orders' in datasets and 'forecasts' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    forecasts_df = datasets['forecasts']\n",
    "    \n",
    "    print(f\"\\nüéØ Orders vs Forecasts Analysis:\")\n",
    "    \n",
    "    # Common products\n",
    "    common_products = set(orders_df['product_id'].unique()) & set(forecasts_df['product_id'].unique())\n",
    "    print(f\"   Common products: {len(common_products):,}\")\n",
    "    print(f\"   Orders-only products: {len(set(orders_df['product_id'].unique()) - common_products):,}\")\n",
    "    print(f\"   Forecasts-only products: {len(set(forecasts_df['product_id'].unique()) - common_products):,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Cross-dataset analysis completed!\")\n",
    "print(f\"üìà Generated {len(cross_insights)} cross-dataset insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis for orders\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders'].copy()\n",
    "    \n",
    "    if 'order_date' in orders_df.columns:\n",
    "        orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "        \n",
    "        print(\"üìÖ TIME SERIES ANALYSIS - ORDERS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Daily trends\n",
    "        daily_metrics = orders_df.groupby('order_date').agg({\n",
    "            'order_value': ['sum', 'mean', 'count'],\n",
    "            'qty': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        daily_metrics.columns = ['daily_revenue', 'avg_order_value', 'order_count', 'total_qty']\n",
    "        \n",
    "        print(f\"üìä Daily Metrics Summary:\")\n",
    "        print(f\"   Date Range: {orders_df['order_date'].min()} to {orders_df['order_date'].max()}\")\n",
    "        print(f\"   Total Days: {len(daily_metrics):,}\")\n",
    "        print(f\"   Average Daily Revenue: R{daily_metrics['daily_revenue'].mean():,.2f}\")\n",
    "        print(f\"   Average Daily Orders: {daily_metrics['order_count'].mean():.1f}\")\n",
    "        \n",
    "        # Weekly patterns\n",
    "        orders_df['day_of_week'] = orders_df['order_date'].dt.day_name()\n",
    "        weekly_pattern = orders_df.groupby('day_of_week').agg({\n",
    "            'order_value': ['sum', 'mean'],\n",
    "            'order_id': 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        weekly_pattern.columns = ['total_revenue', 'avg_order_value', 'order_count']\n",
    "        \n",
    "        # Reorder by weekday\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        weekly_pattern = weekly_pattern.reindex(day_order)\n",
    "        \n",
    "        print(f\"\\nüìÖ Weekly Pattern:\")\n",
    "        print(weekly_pattern)\n",
    "        \n",
    "        # Best and worst days\n",
    "        best_day = weekly_pattern['total_revenue'].idxmax()\n",
    "        worst_day = weekly_pattern['total_revenue'].idxmin()\n",
    "        \n",
    "        print(f\"\\nüèÜ Best performing day: {best_day}\")\n",
    "        print(f\"üìâ Lowest performing day: {worst_day}\")\n",
    "        \n",
    "        # Monthly trends\n",
    "        monthly_trends = orders_df.groupby(orders_df['order_date'].dt.to_period('M')).agg({\n",
    "            'order_value': 'sum',\n",
    "            'order_id': 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        monthly_trends.columns = ['monthly_revenue', 'monthly_orders']\n",
    "        monthly_trends['growth_rate'] = monthly_trends['monthly_revenue'].pct_change() * 100\n",
    "        \n",
    "        print(f\"\\nüìà Monthly Growth Analysis:\")\n",
    "        print(monthly_trends)\n",
    "        \n",
    "        avg_growth = monthly_trends['growth_rate'].mean()\n",
    "        print(f\"\\nüìä Average Monthly Growth Rate: {avg_growth:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Orders dataset not available for time series analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualizations\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders'].copy()\n",
    "    \n",
    "    if 'order_date' in orders_df.columns and 'order_value' in orders_df.columns:\n",
    "        orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "        fig.suptitle('üìÖ Time Series Analysis - Orders Dataset', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Daily revenue trend\n",
    "        daily_revenue = orders_df.groupby('order_date')['order_value'].sum()\n",
    "        axes[0, 0].plot(daily_revenue.index, daily_revenue.values, color=AD_COLORS['primary'], linewidth=2)\n",
    "        axes[0, 0].set_title('Daily Revenue Trend')\n",
    "        axes[0, 0].set_xlabel('Date')\n",
    "        axes[0, 0].set_ylabel('Revenue (R)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Weekly pattern\n",
    "        orders_df['day_of_week'] = orders_df['order_date'].dt.day_name()\n",
    "        weekly_revenue = orders_df.groupby('day_of_week')['order_value'].sum()\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        weekly_revenue = weekly_revenue.reindex(day_order)\n",
    "        \n",
    "        axes[0, 1].bar(weekly_revenue.index, weekly_revenue.values, color=AD_COLORS['secondary'])\n",
    "        axes[0, 1].set_title('Revenue by Day of Week')\n",
    "        axes[0, 1].set_xlabel('Day of Week')\n",
    "        axes[0, 1].set_ylabel('Total Revenue (R)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Monthly trend\n",
    "        monthly_revenue = orders_df.groupby(orders_df['order_date'].dt.to_period('M'))['order_value'].sum()\n",
    "        axes[1, 0].plot(monthly_revenue.index.astype(str), monthly_revenue.values, \n",
    "                       marker='o', color=AD_COLORS['accent'], linewidth=3, markersize=8)\n",
    "        axes[1, 0].set_title('Monthly Revenue Trend')\n",
    "        axes[1, 0].set_xlabel('Month')\n",
    "        axes[1, 0].set_ylabel('Revenue (R)')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Rolling average\n",
    "        daily_revenue_rolling = daily_revenue.rolling(window=7).mean()\n",
    "        axes[1, 1].plot(daily_revenue.index, daily_revenue.values, alpha=0.3, color=AD_COLORS['neutral'], label='Daily')\n",
    "        axes[1, 1].plot(daily_revenue_rolling.index, daily_revenue_rolling.values, \n",
    "                       color=AD_COLORS['highlight'], linewidth=3, label='7-Day Moving Average')\n",
    "        axes[1, 1].set_title('Revenue Trend with Moving Average')\n",
    "        axes[1, 1].set_xlabel('Date')\n",
    "        axes[1, 1].set_ylabel('Revenue (R)')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "print(\"üéØ KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = []\n",
    "recommendations = []\n",
    "\n",
    "# Data Quality Insights\n",
    "print(\"\nüìä DATA QUALITY INSIGHTS:\")\n",
    "total_records = sum(len(df) for df in datasets.values())\n",
    "total_missing = sum(df.isnull().sum().sum() for df in datasets.values())\n",
    "missing_pct = (total_missing / total_records) * 100 if total_records > 0 else 0\n",
    "\n",
    "print(f\"   ‚Ä¢ Data Completeness: {100-missing_pct:.1f}% ({total_records:,} total records)\")\n",
    "if missing_pct < 1:\n",
    "    print(\"   ‚úÖ Excellent data quality - minimal missing values\")\n",
    "    insights.append(\"High data quality with minimal missing values\")\n",
    "elif missing_pct < 5:\n",
    "    print(\"   ‚ö†Ô∏è Good data quality - some missing values to address\")\n",
    "    recommendations.append(\"Implement data validation rules to prevent missing values\")\n",
    "else:\n",
    "    print(\"   ‚ùå Data quality issues - significant missing values detected\")\n",
    "    recommendations.append(\"Prioritize data quality improvement initiatives\")\n",
    "\n",
    "# Business Performance Insights\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    \n",
    "    print(\"\nüí∞ BUSINESS PERFORMANCE INSIGHTS:\")\n",
    "    \n",
    "    if 'order_value' in orders_df.columns:\n",
    "        total_revenue = orders_df['order_value'].sum()\n",
    "        avg_order_value = orders_df['order_value'].mean()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Total Revenue: R{total_revenue:,.2f}\")\n",
    "        print(f\"   ‚Ä¢ Average Order Value: R{avg_order_value:,.2f}\")\n",
    "        \n",
    "        if avg_order_value > 10000:\n",
    "            print(\"   üíé High-value customer base identified\")\n",
    "            insights.append(\"Strong high-value customer segment\")\n",
    "            recommendations.append(\"Develop premium customer retention programs\")\n",
    "    \n",
    "    if 'customer_type' in orders_df.columns:\n",
    "        customer_revenue = orders_df.groupby('customer_type')['order_value'].sum()\n",
    "        top_segment = customer_revenue.idxmax()\n",
    "        top_segment_pct = (customer_revenue.max() / customer_revenue.sum()) * 100\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Top Revenue Segment: {top_segment} ({top_segment_pct:.1f}%)\")\n",
    "        \n",
    "        if top_segment_pct > 50:\n",
    "            print(f\"   ‚ö†Ô∏è High dependency on {top_segment} segment\")\n",
    "            recommendations.append(f\"Diversify customer base beyond {top_segment} segment\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Well-balanced customer portfolio\")\n",
    "            insights.append(\"Balanced customer segment distribution\")\n",
    "\n",
    "# Inventory Insights\n",
    "if 'inventory' in datasets:\n",
    "    inventory_df = datasets['inventory']\n",
    "    \n",
    "    print(\"\nüìã INVENTORY INSIGHTS:\")\n",
    "    \n",
    "    if 'stock_status' in inventory_df.columns:\n",
    "        excess_pct = (inventory_df['stock_status'] == 'Excess').mean() * 100\n",
    "        low_pct = (inventory_df['stock_status'] == 'Low').mean() * 100\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Excess Stock: {excess_pct:.1f}% of products\")\n",
    "        print(f\"   ‚Ä¢ Low Stock: {low_pct:.1f}% of products\")\n",
    "        \n",
    "        if excess_pct > 30:\n",
    "            print(\"   ‚ö†Ô∏è High excess inventory levels detected\")\n",
    "            recommendations.append(\"Implement inventory optimization strategies\")\n",
    "            recommendations.append(\"Review demand planning accuracy\")\n",
    "        \n",
    "        if low_pct > 20:\n",
    "            print(\"   ‚ö†Ô∏è Risk of stockouts - many products at low levels\")\n",
    "            recommendations.append(\"Enhance safety stock calculations\")\n",
    "    \n",
    "    if 'inventory_value' in inventory_df.columns:\n",
    "        total_inventory_value = inventory_df['inventory_value'].sum()\n",
    "        print(f\"   ‚Ä¢ Total Inventory Investment: R{total_inventory_value:,.2f}\")\n",
    "\n",
    "# Forecast Performance Insights\n",
    "if 'forecasts' in datasets:\n",
    "    forecasts_df = datasets['forecasts']\n",
    "    \n",
    "    print(\"\nüéØ FORECAST PERFORMANCE INSIGHTS:\")\n",
    "    \n",
    "    if 'variance_pct' in forecasts_df.columns:\n",
    "        forecasts_df['absolute_error'] = abs(forecasts_df['variance_pct'])\n",
    "        overall_accuracy = 100 - forecasts_df['absolute_error'].mean()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Overall Forecast Accuracy: {overall_accuracy:.1f}%\")\n",
    "        \n",
    "        if overall_accuracy > 85:\n",
    "            print(\"   ‚úÖ Excellent forecast performance\")\n",
    "            insights.append(\"Strong forecasting capabilities\")\n",
    "        elif overall_accuracy > 75:\n",
    "            print(\"   üìà Good forecast performance with room for improvement\")\n",
    "            recommendations.append(\"Fine-tune forecasting models for better accuracy\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Forecast accuracy needs significant improvement\")\n",
    "            recommendations.append(\"Overhaul forecasting methodology\")\n",
    "            recommendations.append(\"Invest in advanced forecasting tools\")\n",
    "    \n",
    "    if 'forecast_type' in forecasts_df.columns:\n",
    "        type_performance = forecasts_df.groupby('forecast_type')['absolute_error'].mean()\n",
    "        best_method = type_performance.idxmin()\n",
    "        best_accuracy = 100 - type_performance.min()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Best Performing Method: {best_method} ({best_accuracy:.1f}% accuracy)\")\n",
    "        recommendations.append(f\"Expand use of {best_method} forecasting approach\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(recommendations[:5], 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\n‚úÖ EDA COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"üìä Generated {len(insights)} key insights\")\n",
    "print(f\"üéØ Provided {len(recommendations)} strategic recommendations\")\n",
    "print(f\"üìà Ready for advanced modeling and dashboard development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export EDA results\n",
    "print(\"üíæ EXPORTING EDA RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create summary report\n",
    "eda_summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'datasets_analyzed': list(datasets.keys()),\n",
    "    'total_records': sum(len(df) for df in datasets.values()),\n",
    "    'data_quality_score': 100 - missing_pct,\n",
    "    'key_insights': insights[:10],  # Top 10 insights\n",
    "    'recommendations': recommendations[:10]  # Top 10 recommendations\n",
    "}\n",
    "\n",
    "# Add dataset-specific metrics\n",
    "if 'orders' in datasets:\n",
    "    orders_df = datasets['orders']\n",
    "    if 'order_value' in orders_df.columns:\n",
    "        eda_summary['total_revenue'] = orders_df['order_value'].sum()\n",
    "        eda_summary['avg_order_value'] = orders_df['order_value'].mean()\n",
    "        eda_summary['total_orders'] = len(orders_df)\n",
    "\n",
    "if 'inventory' in datasets:\n",
    "    inventory_df = datasets['inventory']\n",
    "    if 'inventory_value' in inventory_df.columns:\n",
    "        eda_summary['total_inventory_value'] = inventory_df['inventory_value'].sum()\n",
    "\n",
    "if 'forecasts' in datasets:\n",
    "    forecasts_df = datasets['forecasts']\n",
    "    if 'variance_pct' in forecasts_df.columns:\n",
    "        eda_summary['forecast_accuracy'] = 100 - abs(forecasts_df['variance_pct']).mean()\n",
    "\n",
    "# Save summary to JSON\n",
    "import json\n",
    "with open('eda_summary_report.json', 'w') as f:\n",
    "    json.dump(eda_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ EDA summary exported to 'eda_summary_report.json'\")\n",
    "\n",
    "# Export cleaned datasets for modeling\n",
    "for name, df in datasets.items():\n",
    "    filename = f'{name}_eda_processed.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"üìä Exported {name} dataset: {filename}\")\n",
    "\n",
    "print(f\"\nüéâ EDA PROCESS COMPLETED!\")\n",
    "print(f\"üìà All insights and data ready for S&OP modeling\")\n",
    "print(f\"üî• Proceed to advanced analytics and dashboard development\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}